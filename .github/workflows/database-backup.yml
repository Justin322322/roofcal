name: Automated Database Backup

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: "0 2 * * *"

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      compress:
        description: "Compress backup"
        required: false
        default: true
        type: boolean

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Generate Prisma Client
        run: npx prisma generate

      - name: Create backup directory
        run: mkdir -p backups

      - name: Create database backup
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: npm run backup:db

      - name: Upload backup to artifacts
        uses: actions/upload-artifact@v4
        with:
          name: database-backup-${{ github.run_number }}
          path: backups/*.sql
          retention-days: 30
          if-no-files-found: error

      # Optional: Upload to cloud storage (uncomment and configure as needed)
      # - name: Upload to AWS S3
      #   env:
      #     AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     AWS_REGION: us-east-1
      #   run: |
      #     aws s3 cp backups/ s3://your-bucket/database-backups/ --recursive

      # - name: Upload to Google Cloud Storage
      #   env:
      #     GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
      #   run: |
      #     echo "$GCP_SA_KEY" | base64 -d > gcp-key.json
      #     gcloud auth activate-service-account --key-file=gcp-key.json
      #     gsutil -m cp backups/*.sql.gz gs://your-bucket/database-backups/
